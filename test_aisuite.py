#!/usr/bin/env python3
"""
å¤šå±¤æ¬¡LLMé‚è¼¯é¡Œé©—è­‰ç³»çµ± - aisuiteç‰ˆæœ¬æ¸¬è©¦è…³æœ¬
æ¸¬è©¦aisuiteçµ±ä¸€æ¥å£çš„åŠŸèƒ½
"""
import asyncio
import os
from colorama import init, Fore, Style

# åˆå§‹åŒ–colorama
init()

from config import Config
from llm_client import LLMClient


def test_aisuite_setup():
    """æ¸¬è©¦aisuiteè¨­ç½®"""
    print(f"{Fore.CYAN}ğŸ”§ æ¸¬è©¦aisuiteè¨­ç½®...{Style.RESET_ALL}")
    
    try:
        import aisuite as ai
        print(f"{Fore.GREEN}âœ… aisuiteæ¨¡çµ„è¼‰å…¥æˆåŠŸ{Style.RESET_ALL}")
        
        client = ai.Client()
        print(f"{Fore.GREEN}âœ… aisuiteå®¢æˆ¶ç«¯å‰µå»ºæˆåŠŸ{Style.RESET_ALL}")
        
        return True
    except ImportError:
        print(f"{Fore.RED}âŒ aisuiteæ¨¡çµ„æœªå®‰è£{Style.RESET_ALL}")
        print(f"{Fore.YELLOW}è«‹é‹è¡Œ: pip install aisuite{Style.RESET_ALL}")
        return False
    except Exception as e:
        print(f"{Fore.RED}âŒ aisuiteè¨­ç½®å¤±æ•—: {str(e)}{Style.RESET_ALL}")
        return False


def test_api_keys():
    """æ¸¬è©¦APIå¯†é‘°é…ç½®"""
    print(f"{Fore.CYAN}ğŸ”‘ æ¸¬è©¦APIå¯†é‘°é…ç½®...{Style.RESET_ALL}")
    
    config = Config()
    
    # æª¢æŸ¥å„å€‹APIå¯†é‘°
    api_keys = {
        'OpenAI': config.OPENAI_API_KEY,
        'Anthropic': config.ANTHROPIC_API_KEY,
        'Google': config.GOOGLE_API_KEY,
        'Groq': config.GROQ_API_KEY,
        'OpenRouter (å‚™ç”¨)': config.OPENROUTER_API_KEY
    }
    
    available_apis = []
    for name, key in api_keys.items():
        if key and key != 'your_*_api_key_here':
            available_apis.append(name)
            print(f"{Fore.GREEN}âœ… {name} APIå¯†é‘°å·²é…ç½®{Style.RESET_ALL}")
        else:
            print(f"{Fore.YELLOW}âš ï¸  {name} APIå¯†é‘°æœªé…ç½®{Style.RESET_ALL}")
    
    if not available_apis:
        print(f"{Fore.RED}âŒ æ²’æœ‰é…ç½®ä»»ä½•APIå¯†é‘°{Style.RESET_ALL}")
        return False
    
    print(f"{Fore.GREEN}âœ… å…±æœ‰ {len(available_apis)} å€‹APIå¯ç”¨{Style.RESET_ALL}")
    return True


async def test_model_connection():
    """æ¸¬è©¦æ¨¡å‹é€£æ¥"""
    print(f"{Fore.CYAN}ğŸ¤– æ¸¬è©¦æ¨¡å‹é€£æ¥...{Style.RESET_ALL}")
    
    llm_client = LLMClient()
    config = Config()
    
    # æ¸¬è©¦é †åºï¼šå„ªå…ˆæ¸¬è©¦ä¾¿å®œå’Œå…è²»çš„æ¨¡å‹
    test_models = [
        'openai/gpt-4o-mini',
        'google/gemini-flash', 
        'groq/llama-3-70b',
        'openai/gpt-4o',
        'anthropic/claude-3-5-sonnet'
    ]
    
    successful_models = []
    
    for model_key in test_models:
        if config.is_model_available(model_key):
            print(f"{Fore.BLUE}æ¸¬è©¦æ¨¡å‹: {model_key}{Style.RESET_ALL}")
            
            try:
                result = await llm_client.call_model(
                    model_key, 
                    "è«‹å›ç­”ï¼š1+1ç­‰æ–¼å¤šå°‘ï¼Ÿ"
                )
                
                if result['success']:
                    print(f"{Fore.GREEN}âœ… {model_key} é€£æ¥æˆåŠŸ{Style.RESET_ALL}")
                    print(f"   å›æ‡‰: {result['response'][:50]}...")
                    successful_models.append(model_key)
                else:
                    print(f"{Fore.RED}âŒ {model_key} é€£æ¥å¤±æ•—: {result['error']}{Style.RESET_ALL}")
                
                # çŸ­æš«å»¶é²é¿å…é€Ÿç‡é™åˆ¶
                await asyncio.sleep(1)
                
            except Exception as e:
                print(f"{Fore.RED}âŒ {model_key} æ¸¬è©¦å‡ºéŒ¯: {str(e)}{Style.RESET_ALL}")
    
    if successful_models:
        print(f"{Fore.GREEN}âœ… å…±æœ‰ {len(successful_models)} å€‹æ¨¡å‹å¯ç”¨{Style.RESET_ALL}")
        return True
    else:
        print(f"{Fore.RED}âŒ æ²’æœ‰å¯ç”¨çš„æ¨¡å‹{Style.RESET_ALL}")
        return False


async def test_simple_logic():
    """æ¸¬è©¦ç°¡å–®é‚è¼¯æ¨ç†"""
    print(f"{Fore.CYAN}ğŸ§  æ¸¬è©¦é‚è¼¯æ¨ç†åŠŸèƒ½...{Style.RESET_ALL}")
    
    test_question = """
å°æ˜èªªï¼š"å¦‚æœä»Šå¤©ä¸‹é›¨ï¼Œæˆ‘å°±ä¸å»å…¬åœ’ã€‚"
ä»Šå¤©å°æ˜å»äº†å…¬åœ’ã€‚
è«‹å•ä»Šå¤©æ˜¯å¦ä¸‹é›¨ï¼Ÿè«‹ç”¨JSONæ ¼å¼å›ç­”ï¼ŒåŒ…å«reasoningå’Œanswerå­—æ®µã€‚
"""
    
    try:
        from system_coordinator import SystemCoordinator
        system = SystemCoordinator()
        
        result = await system.process_question(
            test_question.strip(),
            verbose=False
        )
        
        if result['summary']['overall_success']:
            print(f"{Fore.GREEN}âœ… é‚è¼¯æ¨ç†æ¸¬è©¦æˆåŠŸ{Style.RESET_ALL}")
            decision = result['layer_results']['decision']
            print(f"{Fore.BLUE}ğŸ“‹ æœ€çµ‚ç­”æ¡ˆ: {decision['final_answer']}{Style.RESET_ALL}")
            print(f"{Fore.BLUE}ğŸ“Š ä¿¡å¿ƒç¨‹åº¦: {decision['answer_confidence']:.2%}{Style.RESET_ALL}")
            return True
        else:
            print(f"{Fore.RED}âŒ é‚è¼¯æ¨ç†æ¸¬è©¦å¤±æ•—{Style.RESET_ALL}")
            return False
            
    except Exception as e:
        print(f"{Fore.RED}âŒ é‚è¼¯æ¨ç†æ¸¬è©¦å‡ºéŒ¯: {str(e)}{Style.RESET_ALL}")
        return False


def print_usage_guide():
    """æ‰“å°ä½¿ç”¨æŒ‡å—"""
    print(f"\n{Fore.MAGENTA}{'='*60}{Style.RESET_ALL}")
    print(f"{Fore.MAGENTA}ğŸš€ aisuiteç‰ˆæœ¬ä½¿ç”¨æŒ‡å—{Style.RESET_ALL}")
    print(f"{Fore.MAGENTA}{'='*60}{Style.RESET_ALL}")
    
    print(f"\n{Fore.CYAN}ğŸ“‹ æ¨è–¦é…ç½®ï¼ˆå¤§è¦æ¨¡å¯¦é©—ï¼‰:{Style.RESET_ALL}")
    print(f"1. {Fore.GREEN}OpenAI gpt-4o-mini{Style.RESET_ALL} - æˆæœ¬æ•ˆç›Šæœ€ä½³")
    print(f"2. {Fore.GREEN}OpenAI gpt-4o{Style.RESET_ALL} - è³ªé‡æœ€é«˜")
    print(f"3. {Fore.GREEN}Google Gemini{Style.RESET_ALL} - å…è²»é¡åº¦å¤§")
    print(f"4. {Fore.GREEN}Anthropic Claude{Style.RESET_ALL} - é‚è¼¯æ¨ç†å¼·")
    
    print(f"\n{Fore.CYAN}ğŸ’° æˆæœ¬å„ªåŒ–å»ºè­°:{Style.RESET_ALL}")
    print(f"â€¢ æ—¥å¸¸æ¸¬è©¦: gpt-4o-mini ($0.15/1M tokens)")
    print(f"â€¢ é‡è¦å¯¦é©—: gpt-4o ($2.5/1M tokens)")
    print(f"â€¢ é‚è¼¯æ¨ç†: claude-3.5-sonnet")
    print(f"â€¢ å¿«é€Ÿé©—è­‰: Google Gemini (å…è²»)")
    
    print(f"\n{Fore.CYAN}ğŸ¯ ä½¿ç”¨å‘½ä»¤:{Style.RESET_ALL}")
    print(f"â€¢ å–®é¡Œæ¸¬è©¦: python main.py --question \"ä½ çš„å•é¡Œ\" --verbose")
    print(f"â€¢ æ‰¹é‡è™•ç†: python main.py --batch --verbose")
    print(f"â€¢ æª”æ¡ˆè™•ç†: python main.py --file \"2æ˜Ÿé¡Œç›®/2æ˜Ÿ_1.txt\"")
    
    print(f"\n{Fore.CYAN}âš™ï¸  æ€§èƒ½èª¿å„ª:{Style.RESET_ALL}")
    print(f"â€¢ ä¸¦è¡Œè™•ç†: ä¿®æ”¹config.pyä¸­çš„æ¨¡å‹åˆ—è¡¨")
    print(f"â€¢ é€Ÿåº¦å„ªå…ˆ: ä½¿ç”¨Groqæ¨¡å‹")
    print(f"â€¢ è³ªé‡å„ªå…ˆ: ä½¿ç”¨OpenAI gpt-4o")


async def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    print(f"{Fore.MAGENTA}{'='*60}{Style.RESET_ALL}")
    print(f"{Fore.MAGENTA}ğŸ§ª å¤šå±¤æ¬¡LLMé‚è¼¯é©—è­‰ç³»çµ± - aisuiteç‰ˆæœ¬æ¸¬è©¦{Style.RESET_ALL}")
    print(f"{Fore.MAGENTA}{'='*60}{Style.RESET_ALL}\n")
    
    tests = [
        ("aisuiteè¨­ç½®æ¸¬è©¦", test_aisuite_setup),
        ("APIå¯†é‘°é…ç½®æ¸¬è©¦", test_api_keys),
        ("æ¨¡å‹é€£æ¥æ¸¬è©¦", test_model_connection),
        ("é‚è¼¯æ¨ç†åŠŸèƒ½æ¸¬è©¦", test_simple_logic)
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        print(f"{Fore.YELLOW}ğŸ” é–‹å§‹{test_name}...{Style.RESET_ALL}")
        
        try:
            if asyncio.iscoroutinefunction(test_func):
                success = await test_func()
            else:
                success = test_func()
            
            if success:
                passed += 1
                print(f"{Fore.GREEN}âœ… {test_name}é€šé{Style.RESET_ALL}\n")
            else:
                print(f"{Fore.RED}âŒ {test_name}å¤±æ•—{Style.RESET_ALL}\n")
        
        except Exception as e:
            print(f"{Fore.RED}âŒ {test_name}éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}{Style.RESET_ALL}\n")
    
    # ç¸½çµ
    print(f"{Fore.MAGENTA}{'='*60}{Style.RESET_ALL}")
    print(f"{Fore.MAGENTA}ğŸ“Š æ¸¬è©¦çµæœæ‘˜è¦{Style.RESET_ALL}")
    print(f"{Fore.MAGENTA}{'='*60}{Style.RESET_ALL}")
    
    if passed == total:
        print(f"{Fore.GREEN}ğŸ‰ æ‰€æœ‰æ¸¬è©¦é€šéï¼({passed}/{total}){Style.RESET_ALL}")
        print(f"{Fore.GREEN}aisuiteç‰ˆæœ¬å·²æº–å‚™å°±ç·’ï¼Œå¯ä»¥é–‹å§‹å¤§è¦æ¨¡å¯¦é©—ã€‚{Style.RESET_ALL}")
    else:
        print(f"{Fore.YELLOW}âš ï¸  éƒ¨åˆ†æ¸¬è©¦æœªé€šé ({passed}/{total}){Style.RESET_ALL}")
        print(f"{Fore.YELLOW}è«‹æª¢æŸ¥å¤±æ•—çš„æ¸¬è©¦é …ç›®ä¸¦ä¿®å¾©ç›¸é—œå•é¡Œã€‚{Style.RESET_ALL}")
    
    print_usage_guide()


if __name__ == "__main__":
    asyncio.run(main()) 